GPU Maleoon 910 (Huawei Mate Pro+ 60) Evaluation Matrix_Mult

#define BLOCK_SIZE 16
#define COARSE_SIZE 4

__kernel void matmul_coarsened(const int M, const int N, const int K,
                             const __global float* A,
                             const __global float* B,
                             __global float* C) {

    // Thread identifiers
    int row = get_local_id(0);
    int col = get_local_id(1);
    int globalRow = get_group_id(0) * BLOCK_SIZE + row;
    int globalCol = get_group_id(1) * BLOCK_SIZE + col;

    // Shared memory for tiles of A and B
    __local float Asub[BLOCK_SIZE][BLOCK_SIZE];
    __local float Bsub[BLOCK_SIZE][BLOCK_SIZE];

    // Initialize the accumulation register
    float acc = 0.0f;

    // Loop over tiles (K dimension)
    for (int t = 0; t < (K + BLOCK_SIZE - 1) / BLOCK_SIZE; t++) {

        // Load A tile into shared memory (coarsened)
        int tiledRow = t * BLOCK_SIZE + row;
        int tiledCol = col;
        if (tiledRow < M && tiledCol < K && row % COARSE_SIZE == 0 && col % COARSE_SIZE == 0) {
            Asub[row][col] = A[tiledRow * M + tiledCol];
        }

        // Load B tile into shared memory (coarsened)
        tiledRow = row;
        tiledCol = t * BLOCK_SIZE + col;
        if (tiledCol < N && tiledRow < K && row % COARSE_SIZE == 0 && col % COARSE_SIZE == 0) {
            Bsub[row][col] = B[tiledCol * K + tiledRow];
        }

        // Synchronize threads before computation
        barrier(CLK_LOCAL_MEM_FENCE);

        // Compute the dot product for the current tile (coarsened)
        for (int k = 0; k < BLOCK_SIZE; k += COARSE_SIZE) {
            acc += Asub[row][k] * Bsub[k][col];
        }

        // Synchronize threads after computation
        barrier(CLK_LOCAL_MEM_FENCE);
    }

    // Store the result in global memory (out-of-bounds check)
    if (globalRow < M && globalCol < N) {
        C[globalCol * M + globalRow] = acc;
    }
}





//////////////////////



#define BLOCK_SIZE 16

__kernel void matmul_normal(const int M, const int N, const int K,
                           const __global float* A,
                           const __global float* B,
                           __global float* C) {

    // Thread identifiers
    int row = get_local_id(0);
    int col = get_local_id(1);
    int globalRow = get_group_id(0) * BLOCK_SIZE + row;
    int globalCol = get_group_id(1) * BLOCK_SIZE + col;

    // Shared memory for tiles of A and B
    __local float Asub[BLOCK_SIZE][BLOCK_SIZE];
    __local float Bsub[BLOCK_SIZE][BLOCK_SIZE];

    // Initialize the accumulation register
    float acc = 0.0f;

    // Loop over tiles (K dimension)
    for (int t = 0; t < (K + BLOCK_SIZE - 1) / BLOCK_SIZE; t++) {

        // Load A tile into shared memory
        int tiledRow = t * BLOCK_SIZE + row;
        int tiledCol = col;
        if (tiledRow < M && tiledCol < K) {
            Asub[row][col] = A[tiledRow * M + tiledCol];
        }

        // Load B tile into shared memory
        tiledRow = row;
        tiledCol = t * BLOCK_SIZE + col;
        if (tiledCol < N && tiledRow < K) {
            Bsub[row][col] = B[tiledCol * K + tiledRow];
        }

        // Synchronize threads before computation
        barrier(CLK_LOCAL_MEM_FENCE);

        // Compute the dot product for the current tile
        for (int k = 0; k < BLOCK_SIZE; k++) {
            acc += Asub[row][k] * Bsub[k][col];
        }

        // Synchronize threads after computation
        barrier(CLK_LOCAL_MEM_FENCE);
    }

    // Store the result in global memory (out-of-bounds check)
    if (globalRow < M && globalCol < N) {
        C[globalCol * M + globalRow] = acc;
    }
}


////////////////////////////////////////

#define CEIL_DIV(M, N) (((M) + (N)-1) / (N))

#define BM 8
#define BT 16
#define BK 8
#define TM 6

__kernel void sgemm1DBlocktiling(int M, int N, int K, float alpha,
                                 const __global float* A, const __global float* B, float beta,
                                 __global float* C) {
  // If we flip x and y here we get ~30% less performance for large matrices.
  // The current, 30% faster configuration ensures that blocks with sequential
  // blockIDs access columns of B sequentially, while sharing the same row of A.
  // The slower configuration would share columns of A, but access into B would
  // be non-sequential. So the faster configuration has better spatial locality
  // and hence a greater L2 hit rate.
  const uint cRow = get_group_id(1);
  const uint cCol = get_group_id(0);

  // each warp will calculate 32*TM elements, with 32 being the columnar dim.
  const int threadCol = get_local_id(0) % BM;
  const int threadRow = get_local_id(0) / BM;

  // allocate space for the current blocktile in SMEM
  __local float As[BM * BK];
  __local float Bs[BK * BT];

  // Move blocktile to beginning of A's row and B's column
  A += cRow * BM * K;
  B += cCol * BT;
  C += cRow * BM * N + cCol * BT;

  // todo: adjust this to each thread to load multiple entries and
  // better exploit the cache sizes
  assert(BM * BK == get_local_size(0));
  assert(BT * BK == get_local_size(0));
  const uint innerColA = get_local_id(0) % BK; // warp-level GMEM coalescing
  const uint innerRowA = get_local_id(0) / BK;
  const uint innerColB = get_local_id(0) % BT; // warp-level GMEM coalescing
  const uint innerRowB = get_local_id(0) / BT;

  // allocate thread-local cache for results in registerfile
  float threadResults[TM] = {0.0};

  // outer loop over block tiles
  for (uint bkIdx = 0; bkIdx < K; bkIdx += BK) {
    // populate the SMEM caches
    As[innerRowA * BK + innerColA] = A[innerRowA * K + innerColA];
    Bs[innerRowB * BT + innerColB] = B[innerRowB * N + innerColB];
    barrier(CLK_LOCAL_MEM_FENCE);

    // advance blocktile
    A += BK;
    B += BK * N;

    // calculate per-thread results
    for (uint dotIdx = 0; dotIdx < BK; ++dotIdx) {
      // we make the dotproduct loop the outside loop, which facilitates
      // reuse of the Bs entry, which we can cache in a tmp var.
      float tmpB = Bs[dotIdx * BT + threadCol];
      for (uint resIdx = 0; resIdx < TM; ++resIdx) {
        threadResults[resIdx] +=
            As[(threadRow * TM + resIdx) * BK + dotIdx] * tmpB;
      }
    }
    barrier(CLK_LOCAL_MEM_FENCE);
  }

  // write out the results
  for (uint resIdx = 0; resIdx < TM; ++resIdx) {
    C[(threadRow * TM + resIdx) * N + threadCol] =
        alpha * threadResults[resIdx] +
        beta * C[(threadRow * TM + resIdx) * N + threadCol];
  }
}




